{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display\n",
    "import mistune\n",
    "\n",
    "from langchain import OpenAI\n",
    "from langchain.agents import create_openai_tools_agent, AgentExecutor\n",
    "from langchain.tools import BaseTool\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import AIMessage\n",
    "from langchain_core.outputs import ChatGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "\n",
    "load_dotenv()\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "\n",
    "if not OPENROUTER_API_KEY:\n",
    "    raise ValueError(\"Please set the OPENROUTER_API_KEY environment variable.\")\n",
    "\n",
    "API_URL = \"https://openrouter.ai/api/v1/chat/completions\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Agent\n",
    "\n",
    "def call_openrouter(message, model):\n",
    "\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": message}\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    response = requests.post(API_URL, headers=HEADERS, data=json.dumps(payload))\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if \"choices\" not in data:   \n",
    "            raise Exception(f\"Unexpected response format: {data}\")\n",
    "        return data[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "        return data[\"choices\"][0][\"message\"][\"content\"]\n",
    "    else:\n",
    "        raise Exception(f\"Request failed with status {response.status_code}: {response.text}\")\n",
    "\n",
    "\n",
    "\n",
    "def agent(prompt, model):\n",
    "\n",
    "    print(\"Agent starting...\")\n",
    "    print(\"Sending prompt:\", prompt)\n",
    "    \n",
    "    try:\n",
    "        response_text = call_openrouter(prompt, model)\n",
    "        display(Markdown(response_text))\n",
    "\n",
    "        markdown = mistune.create_markdown(renderer='ast')\n",
    "        tokens = markdown(response_text)\n",
    "        # print(tokens)\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(\"\\nError during API call:\\n\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 Agents\n",
    "\n",
    "def conversation_session(agent1_model, agent2_model, initial_message, num_turns=5):\n",
    "   \n",
    "    conversation_history = []\n",
    "    current_message = initial_message\n",
    "    \n",
    "    print(\"Starting conversation...\\n\")\n",
    "    print(\"Agent1:\", current_message)\n",
    "\n",
    "    conversation_history.append((\"Agent1\", current_message))\n",
    "    \n",
    "\n",
    "    for turn in range(num_turns):\n",
    "        prompt_for_agent2 = \"The conversation so far:\\n\"\n",
    "        for speaker, msg in conversation_history:\n",
    "            prompt_for_agent2 += f\"{speaker}: {msg}\\n\"\n",
    "\n",
    "        prompt_for_agent2 += \"\\nAgent1 just spoke. Agent2, please respond.\"\n",
    "        try:\n",
    "            agent2_response = call_openrouter(prompt_for_agent2, agent2_model)\n",
    "        except Exception as e:\n",
    "            print(f\"Error from Agent2: {e}\")\n",
    "            agent2_response = \"[Error in Agent2 response]\"\n",
    "\n",
    "        print(\"Agent2:\", agent2_response)\n",
    "        conversation_history.append((\"Agent2\", agent2_response))\n",
    "        \n",
    "        prompt_for_agent1 = \"The conversation so far:\\n\"\n",
    "        for speaker, msg in conversation_history:\n",
    "            prompt_for_agent1 += f\"{speaker}: {msg}\\n\"\n",
    "\n",
    "        prompt_for_agent1 += \"\\nAgent2 just spoke. Agent1, please respond.\"\n",
    "        try:\n",
    "            agent1_response = call_openrouter(prompt_for_agent1, agent1_model)\n",
    "        except Exception as e:\n",
    "            print(f\"Error from Agent1: {e}\")\n",
    "            agent1_response = \"[Error in Agent1 response]\"\n",
    "\n",
    "        print(\"Agent1:\", agent1_response)\n",
    "        conversation_history.append((\"Agent1\", agent1_response))\n",
    "    \n",
    "    return conversation_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 1-2 Agents \n",
    "\n",
    "gemini_2_flash_lite = 'google/gemini-2.0-flash-lite-preview-02-05:free'\n",
    "gemini_2_pro_exp = 'google/gemini-2.0-pro-exp-02-05:free'\n",
    "gemini_2_flash_thinking_exp = 'google/gemini-2.0-flash-thinking-exp:free'\n",
    "\n",
    "prompt = 'Discuss building a short term trading strategy for BTCUSDT. Scalping is the goal on the 1-1hr time frames. The strategy should be based on the price and volume data not all these other indicators.\\n'\n",
    "\n",
    "# response = agent(prompt, gemini_2_flash_lite)\n",
    "# history = conversation_session(gemini_2_pro_exp, gemini_2_pro_exp, prompt, num_turns=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Langchain MultiAgent \n",
    "\n",
    "class OpenRouterLLM(LLM):\n",
    "    openrouter_key: str\n",
    "    model_name: str\n",
    "    temperature: float = 0.0  \n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"openrouter\"\n",
    "\n",
    "    def _call(self, prompt: str, stop=None, **kwargs) -> str:\n",
    "        API_URL = \"https://openrouter.ai/api/v1/chat/completions\"\n",
    "        HEADERS = {\n",
    "            \"Authorization\": f\"Bearer {self.openrouter_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        payload = {\n",
    "            \"model\": self.model_name,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "            \"temperature\": self.temperature\n",
    "        }\n",
    "\n",
    "        response = requests.post(API_URL, headers=HEADERS, data=json.dumps(payload))\n",
    "        data = response.json()\n",
    "        output_text = data[\"choices\"][0][\"message\"][\"content\"]\n",
    "        # Wrap into a ChatGeneration if needed for logging or debugging,\n",
    "        # but return only the text:\n",
    "        chat_output = ChatGeneration(message=AIMessage(content=output_text, role=\"assistant\"))\n",
    "        return chat_output.message.content\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self):\n",
    "        return {\n",
    "            \"openrouter_key\": self.openrouter_key,\n",
    "            \"model_name\": self.model_name,\n",
    "            \"temperature\": self.temperature\n",
    "        }\n",
    "\n",
    "\n",
    "llm = OpenRouterLLM(\n",
    "    openrouter_key=OPENROUTER_API_KEY,\n",
    "    model_name=\"google/gemini-2.0-flash-thinking-exp:free\",\n",
    "    temperature=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent Tools\n",
    "\n",
    "# Tool to send emails (placeholder)\n",
    "class EmailTool(BaseTool):\n",
    "    name: str = \"send_email\"\n",
    "    description: str = \"Sends an email with a subject and body to a given recipient.\"\n",
    "\n",
    "    def _run(self, subject: str, body: str, recipient: str) -> str:\n",
    "        # Here you would integrate with your email API.\n",
    "        return f\"Email sent to {recipient} with subject '{subject}' and body '{body[:50]}...'.\"\n",
    "\n",
    "    async def _arun(self, subject: str, body: str, recipient: str) -> str:\n",
    "        raise NotImplementedError(\"Async not implemented\")\n",
    "\n",
    "\n",
    "# Tool to scrape web content\n",
    "class WebScraperTool(BaseTool):\n",
    "    name: str = \"scrape_web\"\n",
    "    description: str = \"Scrapes and returns plain text from the given URL.\"\n",
    "\n",
    "    def _run(self, url: str) -> str:\n",
    "        from bs4 import BeautifulSoup\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        return soup.get_text(separator=\"\\n\")  # Return text with line breaks\n",
    "\n",
    "    async def _arun(self, url: str) -> str:\n",
    "        raise NotImplementedError(\"Async not implemented\")\n",
    "\n",
    "\n",
    "# Tool to retrieve YouTube video details (placeholder)\n",
    "class YouTubeTool(BaseTool):\n",
    "    name: str = \"scrape_youtube\"\n",
    "    description: str = \"Retrieves information about a YouTube video given its URL.\"\n",
    "\n",
    "    def _run(self, url: str) -> str:\n",
    "        # In production, use the YouTube Data API or a dedicated library.\n",
    "        return f\"Details for YouTube video at {url} (dummy data).\"\n",
    "\n",
    "    async def _arun(self, url: str) -> str:\n",
    "        raise NotImplementedError(\"Async not implemented\")\n",
    "\n",
    "\n",
    "# Tool to generate a report from data\n",
    "class ReportGeneratorTool(BaseTool):\n",
    "    name: str = \"generate_report\"\n",
    "    description: str = \"Generates a summary report from the provided data string.\"\n",
    "\n",
    "    def _run(self, data: str) -> str:\n",
    "        # In production, you might use templating or ask the LLM to generate a report.\n",
    "        return f\"Report Summary: {data[:100]}...\"  # Dummy summary using first 100 characters\n",
    "\n",
    "    async def _arun(self, data: str) -> str:\n",
    "        raise NotImplementedError(\"Async not implemented\")\n",
    "\n",
    "\n",
    "\n",
    "tools = [EmailTool(), WebScraperTool(), YouTubeTool(), ReportGeneratorTool()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run MultiAgent \n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a multi-tool agent that can perform a variety of tasks.\"),\n",
    "    MessagesPlaceholder(\"chat_history\", optional=True),\n",
    "    (\"human\", \"{input}\"),\n",
    "    MessagesPlaceholder(\"agent_scratchpad\")\n",
    "])\n",
    "\n",
    "agent = create_openai_tools_agent(llm, tools, prompt=prompt_template)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools)\n",
    "\n",
    "\n",
    "query = (\n",
    "    \"Send an email with subject 'Daily Report' to [email protected] \"\n",
    "    \"that summarizes the latest news from https://cnn.com.\"\n",
    ")\n",
    "\n",
    "result = agent_executor.invoke({\"input\": query})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents-X0DXTgtb-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
